### This essay became outdated quickly. Thanks to NovelAI implementing an easy to use and view log probability viewer, end-user diagnostics have become easy, rapid and affordable. This makes it practical to set up testbench prompts to test the behavior of decoding strategies or text stage instruction strategies. These still apply to a limited context. Finding the one method to rule them all seems unlikely with how variable the goals of different writers may be. The beauty of having research tools and multiple emerging decoding strategies is that different optimized strategies could be developed for specific writing tasks.

## Why repeat testing by users is mostly a waste of time

This is an opinionated essay by birb separated from the main document as it won't help you write better WI and contains jargon on the inner workings machine learning tools and language transformers. Short answer: machines following simple instructions do a vastly better job than humans and AID isn't free. The goal is to convince the reader to be content with their current anecdotal and feeling-based testing methods (side-effects may include depression). Coming from academia and a long hobby background in machine learning, the author thinks he knows a thing or two about the field. Picture generating GANs is where most of the research and available white papers are. Their best attempt at sciencing results comes in the form of comparing the similarity of pixels of the outputted image and Ground Truth. There also exist methods of comparing PSNR to make data-based judgments on. 

Ultimately these are academic justifications. Every time I've seen their papers, the final judgment on how good a model or result is still comes down to user-perception. If your eyes and brain feels that a certain output is desired that's the direction you aim for while the numbers take second seat. When training your model sure you have the epoch, the batch size and the iterations and then you can represent the transformation of pixels (or any given data type) as a matrix, but what the model sitting on your hard disk contains and does in the ML tool remains largely a black box especially for the consumers that end up receiving the resulting tool. This is worse when it comes to writing. The GPT-3 as such isn't the worst about it: it's simply trying to produce natural and fluent sounding English as it was made to do. But then we get into the applications built on GPT-3 that have different goals.

Think about what AID is and is officially sold as: The non-linear infinite text adventure where you choose your adventure with the application (the GPT-3 based model) being the dungeon master. Everything about that tells you it must be designed to be random (temperate in ML terms) to achieve its goal. On premium you get some limited control over this. WI writers to try to control or restrict this randomness by defining WI where certain objects are given traits the author desires to remain universal. Zynj's jsonthing has achieved this goal to a surprising degree of accuracy by overwriting the WI-location and adding new features into it. Regardless, most of AID's scripts are set up in a way to increase variability and reduce repetition while also maintaining GPT-3's natural writing aspect. Since it is a text adventure, during training heavy weight/bias was given to CYOA style sources like chooseyourstory.com(official words from devs). This can be inferred from characters like Blooded's Count Grey being such a common sight in random AID stories.

Here we must borrow from and paraphrase a few public documents:

http://gptprompts.wikidot.com/intro:logprobs

https://github.com/cloveranon/Clover-Edition/blob/master/gpt2generator.py

https://github.com/cloveranon/Clover-Edition/blob/master/config.ini

https://aidungeon.featureupvote.com/suggestions/102773/repetition-penalty-similar-to-what-the-clover-edition-fork-had

We go up in reverse order. Official statements confirm that the online AID features repetition penalty that was originally introduced in forks of the open source implementation. Despite being GPT-2 and basically dead Clover-Edition is featured because it has the easiest to demonstrate example of this. The config.ini explains roughly what the penalty does and how it can be set. Within the gpt2generator.py you can see the scripts that relate to repetition penalty(ctrl+f 'rep'). The benefit of this is discouraging the AI from looping. It may or may not effect retry, but you have to understand the source code(if it's available) to ascertain this. Modern AID does all of this API-side and is undocumented. The point remains that AID output generation is; temperate, non-deterministic and has a repetition penalty. Ever wondered why you got your colors right on first two rerolls, then the rerolls got worse? This is likely why.

To demonstrate the level of randomness and non-determinism in AID we have to understand GPT-3 that the models are based on and logprobs. It is recommended you read the linked wikipage as it does a better job explaining than the author could. Put simply at 0 temperature the model would be deterministic. Whatever word AI thinks most likely follows by logprob will be outputted. In AID you are allowed 0.5-2.0 randomness. It is unclear what this means in temp, but certainly AID will never use 0 temp. It will always feature randomness, this is expected and entirely desired for a text adventure. Going by the logprob article, temperature is the measure of differentiation (fuzziness) and introducing more suboptimal options into the set of acceptable outputs. Most white papers in the field recommend 0.9 temp for academic text transformation purposes. When you run the algorithm with a temp variable, you will start getting pseudo-random results. All AID players know this.

What the combination of all these things means is that the author can argue that trying to scientifically and probabilistically measure WI outputs as a sole human is a waste of time and your valuable Dragon energy. You're not going to escape medium temp and the rep penalty (also called a discriminator). The scripts kick in and twist your results far before you can produce a scientifically valid amount of tests (N=100 if we're being generous). With AI unit testing or a machine script you could run a controlled trial thousands of times to where even with all these mysterious formulas you could calculate useful probabilities. The only ones who have the resources to achieve this would be Latitude or OpenAI. Most users can only run binomial tests in the format of define your data(WI, A/N, remember) then keep outputting with continue or retry your action. Repeat 100 times for each set then continue with another set. This goes against the design of AID and would be relevant only at low temp values and rep penalty turned off. 

Binomial repeat testing is a wasteful of time and energy without scripting due to how far back WI goes. It might be feasible if you're good at botting and have OpenAI beta access and have the skills to do statistical regression analysis but that's not most people reading this document. The author supports all materials pertaining to writing better stories with AID, but leaves other users to write and provide their own research materials and proof of worth online.

Despite all of the above we claim anecdotal trials with WI is worth your time. This is how Zaltys honed his format, zynj completed his scripts and and birb achieved damn near 100% output on his unique costumes. The closer to the front the WI is and the more unique(the less data GPT-3 has to play with) it is, the more consistent your output. This might be a lot to take in but it shouldn't discourage you. We shared our WI tricks, we wrote this document and many people produced more fun and consistent results in AID. If you find a personally desirable result that works for you use and hone it. 

The author supports people sharing their testing regardless. It reveals the things that definitely don't work and the things that seem to do something. This is fastest discernible if you place WI at the front using JSONthing. Monky used a method of testing that brought out greatly differentiated results when he was making the Neanderthal format. If results are provided with permission, birb will gladly share them in his repository. The author recommends you read Latitude's articles at medium.com when you have time or feel bored, they are quite interesting.

https://aidungeon.medium.com
